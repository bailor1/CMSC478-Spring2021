{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### <h1><center>CMSC 478: Introduction to Machine Learning</center></h1>\n",
    "\n",
    "<center><img src=\"img/title.jpg\" align=\"center\"/></center>\n",
    "\n",
    "\n",
    "<h3 style=\"color:blue;\"><center>Instructor: Fereydoon Vafaei</center></h3>\n",
    "\n",
    "\n",
    "<h5 style=\"color:purple;\"><center>Dimensionality Reduction and PCA</center></h5>\n",
    "\n",
    "<center><img src=\"img/UMBC_logo.png\" align=\"center\"/></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h1><center>Agenda</center></h1>\n",
    "\n",
    "\n",
    "- Curse of Dimensionality\n",
    "- <b> Dimensionality Reduction</b>\n",
    "    - Manifold Learning\n",
    "    - Data Projection\n",
    "        - Reducing Reconstruction Error\n",
    "        - Maximizing Variance\n",
    "- Principal Components Analysis (PCA)\n",
    "    - Singular Value Decomposition (SVD)\n",
    "- Explained Variance Ratio\n",
    "- PCA for Compression\n",
    "- Kernel PCA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h1><center>Curse of Dimensionality</center></h1>\n",
    "\n",
    "- We are so used to living in three dimensions that our intuition fails us when we try to imagine a high-dimensional space.\n",
    "\n",
    "- Even a basic 4D hypercube is incredibly hard to picture in our minds.\n",
    "\n",
    "- Watch a rotating [tesseract](https://en.wikipedia.org/wiki/Tesseract) projected into 3D space at https://homl.info/30.\n",
    "\n",
    "<center><img src=\"img/fig-8-1.png\" align=\"center\"/></center>\n",
    "\n",
    "<font size='1'>Image from Ref[1]</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h1><center>Curse of Dimensionality - High-Dimensional Space</center></h1>\n",
    "\n",
    "- It turns out that many things behave very differently in **high-dimensional space**.\n",
    "\n",
    "\n",
    "- For example, if you pick a random point in a unit square (a 1 × 1 square), it will have only about a 0.4% chance of being located less than 0.001 from a border (in other words, it is very unlikely that a random point will be “extreme” along any dimension).\n",
    "\n",
    "\n",
    "- But in a 10,000-dimensional unit hypercube, this probability is greater than 99.999999%.\n",
    "\n",
    "\n",
    "- Most points in a high-dimensional hypercube are very close to the border."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h1><center>Curse of Dimensionality - Sparsity</center></h1>\n",
    "\n",
    "- Here is a more troublesome difference: if you pick two points randomly in a unit square, the distance between these two points will be, on average, roughly 0.52. If you pick two random points in a unit 3D cube, the average distance will be roughly 0.66.\n",
    "\n",
    "- But what about two points picked randomly in a 1,000,000-dimensional hypercube? The average distance will be about 408.25 roughly $\\sqrt{1, 000, 000/6}$\n",
    "\n",
    "- This is counterintuitive: how can two points be so far apart when they both lie within the same unit hypercube? Well, there’s just plenty of space in high dimensions. As a result, high-dimensional datasets are at risk of being very **sparse**: most training instances are likely to be far away from each other.\n",
    "\n",
    "- This also means that a new instance will likely be far away from any training instance, making predictions much less reliable than in lower dimensions, since they will be based on much larger extrapolations.\n",
    "\n",
    "- In short, the more dimensions the training set has, the greater the risk of **overfitting** it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h1><center>Curse of Dimensionality - Increasing the Size of Training Set</center></h1>\n",
    "\n",
    "- In theory, one solution to the curse of dimensionality could be to increase the size of the training set to reach a sufficient density of training instances.\n",
    "\n",
    "\n",
    "- Unfortunately, in practice, the number of training instances required to reach a given density grows exponentially with the number of dimensions. With just 100 features (significantly fewer than in the MNIST problem), you would need more training instances than atoms in the observable universe in order for training instances to be within 0.1 of each other on average, assuming they were spread out uniformly across all dimensions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h1><center>Curse of Dimensionality - Computational Costs</center></h1>\n",
    "\n",
    "<center><img src=\"img/curse.png\" align=\"center\"/></center>\n",
    "\n",
    "<font size='1'>Image from Ref[2]</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h1><center>Curse of Dimensionality - Computational Costs</center></h1>\n",
    "\n",
    "<center><img src=\"img/curse-2.png\" align=\"center\"/></center>\n",
    "\n",
    "<font size='1'>Image from Ref[2]</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h1><center>High-Dimensional Data</center></h1>\n",
    "\n",
    "### High Dimensions - Lots of features\n",
    "\n",
    "\n",
    "<center><img src=\"img/meg.png\" align=\"center\"/></center>\n",
    "\n",
    "<font size='1'>Image from Ref[3]</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h1><center>High-Dimensional Data - fMRI</center></h1>\n",
    "\n",
    "<center><img src=\"img/fmri_scanner.jpeg\" align=\"center\"/></center>\n",
    "\n",
    "<center><img src=\"img/fmri_scan.jpg\" align=\"center\"/></center>\n",
    "\n",
    "<font size='1'>Images from Ref[6]</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h1><center>Dimensionality Reduction</center></h1>\n",
    "\n",
    "- In many practical applications of ML, it is desirable to reduce the **dimensionality** of the data:\n",
    "    - Data visualization\n",
    "    - Data exploration: for investigating the **effective** dimensionality of the data\n",
    "    - More efficient use of resources (e.g., time, memory, communication)\n",
    "    - Overfitting: fewer dimensions -> better generalization\n",
    "    - Noise removal (improving data quality)\n",
    "    - Preprocessing: further processing by ML models (e.g. image classifiers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h1><center>Main Approaches for Dimensionality Reduction</center></h1>\n",
    "\n",
    "- There are two main approaches to reducing dimensionality:\n",
    "    - **Projection**\n",
    "    - **Manifold Learning**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h1><center>Projection</center></h1>\n",
    "\n",
    "- In most real-world problems, training instances are not spread out uniformly across all dimensions. Many features are almost constant, while others are highly correlated. As a result, all training instances lie within (or close to) a much lower-dimensional subspace of the high-dimensional space.\n",
    "\n",
    "- In Figure 8-2 you can see a 3D dataset represented by circles.\n",
    "\n",
    "- Notice that all training instances lie close to a plane: this is a lower-dimensional (2D) subspace of the high-dimensional (3D) space.\n",
    "\n",
    "<center><img src=\"img/fig-8-2.png\" align=\"center\"/></center>\n",
    "\n",
    "<font size='1'>Image from Ref[1]</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h1><center>Projection</center></h1>\n",
    "\n",
    "- If we project every training instance perpendicularly onto this subspace (as represented by the short lines connecting the instances to the plane), we get the new 2D dataset shown in Figure 8-3.\n",
    "\n",
    "- We have just reduced the dataset’s dimensionality from 3D to 2D. Note that the axes correspond to new features $z_1$ and $z_2$ (the coordinates of the projections on the plane).\n",
    "\n",
    "<center><img src=\"img/fig-8-3.png\" align=\"center\"/></center>\n",
    "\n",
    "<font size='1'>Image from Ref[1]</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h1><center>Swiss Roll - 2D Manifold</center></h1>\n",
    "\n",
    "- Projection is not always the best approach to dimensionality reduction.\n",
    "\n",
    "- In many cases the subspace may twist and turn, such as in the famous **Swiss roll** toy dataset represented in Figure 8-4.\n",
    "\n",
    "- The Swiss roll is an example of a **2D manifold**. Put simply, a **2D manifold** is a 2D shape that can be bent and twisted in a higher-dimensional space.\n",
    "\n",
    "- More generally, a d-dimensional manifold is a part of an n-dimensional space (where d < n) that locally resembles a d-dimensional hyperplane.\n",
    "\n",
    "- In the case of the Swiss roll, d = 2 and n = 3: it locally resembles a 2D plane, but it is rolled in the third dimension.\n",
    "\n",
    "<center><img src=\"img/fig-8-4.png\" align=\"center\"/></center>\n",
    "\n",
    "<font size='1'>Image from Ref[1]</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h1><center>Unrolling Swiss Roll Manifold</center></h1>\n",
    "\n",
    "- Simply projecting onto a plane (e.g., by dropping $x_3$) would squash different layers of the Swiss roll together, as shown on the left side of Figure 8-5.\n",
    "\n",
    "- What you really want is to unroll the Swiss roll to obtain the 2D dataset on the right side of Figure 8-5.\n",
    "\n",
    "<center><img src=\"img/fig-8-5.png\" align=\"center\"/></center>\n",
    "\n",
    "<font size='1'>Image from Ref[1]</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h1><center>Manifold Learning</center></h1>\n",
    "\n",
    "- Many dimensionality reduction algorithms work by modeling the manifold on which the training instances lie; this is called **Manifold Learning**.\n",
    "\n",
    "\n",
    "- It relies on the manifold assumption, also called the **manifold hypothesis**, which holds that most real-world high-dimensional datasets lie close to a much lower-dimensional manifold.\n",
    "\n",
    "\n",
    "- This assumption is very often empirically observed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h1><center>Manifold Learning Assumptions</center></h1>\n",
    "\n",
    "- The **manifold assumption** is often accompanied by another implicit assumption: that the task at hand (e.g., classification or regression) will be simpler if expressed in the lower-dimensional space of the manifold.\n",
    "\n",
    "- For example, in the top row of Figure 8-6 the Swiss roll is split into two classes: in the 3D space (on the left), the decision boundary would be fairly complex, but in the 2D unrolled manifold space (on the right), the decision boundary is a straight line.\n",
    "\n",
    "- However, this implicit assumption does not always hold. For example, in the bottom row of Figure 8-6, the decision boundary is located at $x_1 = 5$. This decision boundary looks very simple in the original 3D space (a vertical plane), but it looks more complex in the unrolled manifold (a collection of four independent line segments).\n",
    "\n",
    "- In short, reducing the dimensionality of your training set before training a model will usually speed up training, but it may not always lead to a better or simpler solution; it all depends on the dataset.\n",
    "\n",
    "<center><img src=\"img/fig-8-6.png\" align=\"center\"/></center>\n",
    "\n",
    "<font size='1'>Image from Ref[1]</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h1><center>Data Projection</center></h1>\n",
    "\n",
    "<center><img src=\"img/projection-01.png\" align=\"center\"/></center>\n",
    "\n",
    "<font size='1'>Slide from Ref[5]</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h1><center>Dimensionality Reduction - Reconstruction</center></h1>\n",
    "\n",
    "<center><img src=\"img/dimensionality-reduction.png\" align=\"center\"/></center>\n",
    "\n",
    "<font size='1'>Slide from Ref[5]</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h1><center>Principal Component Analysis (PCA) - Geometric Perspective</center></h1>\n",
    "\n",
    "<center><img src=\"img/pca-def.png\" align=\"center\"/></center>\n",
    "\n",
    "<font size='1'>Slide from Ref[5]</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h1><center>PCA - Problem Definition</center></h1>\n",
    "\n",
    "<center><img src=\"img/pca-problem.png\" align=\"center\"/></center>\n",
    "\n",
    "<font size='1'>Slide from Ref[5]</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h1><center>First Principal Component</center></h1>\n",
    "\n",
    "<center><img src=\"img/pca-first.png\" align=\"center\"/></center>\n",
    "\n",
    "<font size='1'>Slide from Ref[5]</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h1><center>Equivalency of Minimizing Reconstruction Error and Maximizing the Variance</center></h1>\n",
    "\n",
    "<center><img src=\"img/pca-eq.png\" align=\"center\"/></center>\n",
    "\n",
    "<font size='1'>Slide from Ref[5]</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h1><center>Reconstruction and Variance</center></h1>\n",
    "\n",
    "<center><img src=\"img/max-variance.png\" align=\"center\"/></center>\n",
    "\n",
    "<font size='1'>Slide from Ref[5]</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h1><center>Centering</center></h1>\n",
    "\n",
    "<center><img src=\"img/centering.png\" align=\"center\"/></center>\n",
    "\n",
    "<font size='1'>Slide from Ref[5]</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h1><center>Centered Data</center></h1>\n",
    "\n",
    "<center><img src=\"img/projection-02.png\" align=\"center\"/></center>\n",
    "\n",
    "<font size='1'>Image from Ref[4]</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h1><center>Centered Data Projection</center></h1>\n",
    "\n",
    "<center><img src=\"img/projection-3.png\" align=\"center\"/></center>\n",
    "\n",
    "<font size='1'>Image from Ref[4]</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h1><center>Projecting Data to Reduce Dimensionality</center></h1>\n",
    "\n",
    "- There are infinite number of choices (directions) for projecting the data; thus we need a criteria:\n",
    "\n",
    "    - Finding a direction with <font color=\"blue\"><b>Minimum Reconstruction Error</b></font> (minimum information loss)\n",
    "\n",
    "    - Equivalently, finding a direction with <font color=\"blue\"><b>Maximum Variance</b></font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h1><center>Data Projection - Minimum Reconstruction Error</center></h1>\n",
    "\n",
    "<center><img src=\"img/reconstruction.png\" align=\"center\"/></center>\n",
    "\n",
    "<font size='1'>Image from Ref[2]</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h1><center>Data Projection - Maximizing the Variance</center></h1>\n",
    "\n",
    "### Consider the two projections below. Which one maximizes the variance?\n",
    "\n",
    "<center><img src=\"img/variance-1.png\" align=\"center\"/></center>\n",
    "\n",
    "<font size='1'>Image from Ref[3]</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h1><center>Preserving the Variance</center></h1>\n",
    "\n",
    "- Before you can project the training set onto a lower-dimensional hyperplane, you first need to choose the right hyperplane. For example, a simple 2D dataset is represented on the left in Figure 8-7, along with three different axes (i.e., 1D hyperplanes).\n",
    "\n",
    "- On the right is the result of the projection of the dataset onto each of these axes. As you can see, the projection onto the solid line preserves the maximum variance, while the projection onto the dotted line preserves very little variance and the projection onto the dashed line preserves an intermediate amount of variance.\n",
    "\n",
    "- It seems reasonable to select the axis that **preserves the maximum amount of variance**, as it will most likely lose less information than the other projections.\n",
    "\n",
    "- Another way to justify this choice is that it is the axis that minimizes the mean squared distance between the original dataset and its projection onto that axis. This is the rather simple idea behind PCA.\n",
    "\n",
    "<center><img src=\"img/fig-8-7.png\" align=\"center\"/></center>\n",
    "\n",
    "<font size='1'>Image from Ref[1]</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h1><center>PCA 1st Axis</center></h1>\n",
    "\n",
    "<center><img src=\"img/variance-2.png\" align=\"center\"/></center>\n",
    "\n",
    "<font size='1'>Image from Ref[3]</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h1><center>PCA 2nd Axis</center></h1>\n",
    "\n",
    "<center><img src=\"img/variance-3.png\" align=\"center\"/></center>\n",
    "\n",
    "<font size='1'>Image from Ref[3]</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "<h1><center>Eigenproblem</center></h1>\n",
    "\n",
    "<center><img src=\"img/eigen-1.png\" align=\"center\"/></center>\n",
    "\n",
    "<font size='1'>Slide from Ref[5]</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "<h1><center>Eigenproblem Cont.</center></h1>\n",
    "\n",
    "<center><img src=\"img/eigen-2.png\" align=\"center\"/></center>\n",
    "\n",
    "<font size='1'>Slide from Ref[5]</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "<h1><center>Eigenproblem Cont.</center></h1>\n",
    "\n",
    "<center><img src=\"img/eigen-3.png\" align=\"center\"/></center>\n",
    "\n",
    "<font size='1'>Slide from Ref[5]</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "<h1><center>Eigenproblem Cont.</center></h1>\n",
    "\n",
    "<center><img src=\"img/eigen-4.png\" align=\"center\"/></center>\n",
    "\n",
    "<font size='1'>Slide from Ref[5]</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h1><center>Principal Components</center></h1>\n",
    "\n",
    "- How can you find the **principal components** of a training set?\n",
    "\n",
    "- Luckily, there is a standard matrix factorization technique called **Singular Value Decomposition (SVD)** that can decompose the training set matrix $\\mathbf{X}$ into the matrix multiplication of three matrices $\\mathbf{U Σ} \\mathbf{V}^T$ where $\\mathbf{V}$ contains the unit vectors that define all the principal components that we are looking for, as shown in Equation 8-1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "**Equation 8-1: Principal components matrix**\n",
    "\n",
    "$\n",
    "\\mathbf{V}^T =\n",
    "\\begin{pmatrix}\n",
    "  \\mid & \\mid & & \\mid \\\\\n",
    "  \\mathbf{c_1} & \\mathbf{c_2} & \\cdots & \\mathbf{c_n} \\\\\n",
    "  \\mid & \\mid & & \\mid\n",
    "\\end{pmatrix}\n",
    "$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h1><center>Singular Value Decomposition (SVD)</center></h1>\n",
    "\n",
    "<center><img src=\"img/svd-1.png\" align=\"center\"/></center>\n",
    "\n",
    "<font size='1'>Slide from Ref[5]</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "<h1><center>SVD Cont.</center></h1>\n",
    "\n",
    "<center><img src=\"img/svd-2.png\" align=\"center\"/></center>\n",
    "\n",
    "<font size='1'>Slide from Ref[5]</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "<h1><center>SVD Cont.</center></h1>\n",
    "\n",
    "<center><img src=\"img/svd-3.png\" align=\"center\"/></center>\n",
    "\n",
    "<font size='1'>Slide from Ref[5]</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h1><center>Direction of Principal Components</center></h1>\n",
    "\n",
    "- For each principal component, PCA finds a zero-centered unit vector pointing in the direction of the PC.\n",
    "\n",
    "- Since two opposing unit vectors lie on the same axis, the direction of the unit vectors returned by PCA is not stable: if you perturb the training set slightly and run PCA again, the unit vectors may point in the opposite direction as the original vectors. However, they will generally still lie on the same axes.\n",
    "\n",
    "- In some cases, a pair of unit vectors may even rotate or swap (if the variances along these two axes are close), but the plane they define will generally remain the same.\n",
    "\n",
    "<center><img src=\"img/pca-directions.png\" align=\"center\"/></center>\n",
    "\n",
    "<font size='1'>Image from Ref[7]</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h1><center>Projecting Down to d Dimensions</center></h1>\n",
    "\n",
    "- Once you have identified all the principal components, you can reduce the dimensionality of the dataset down to $d$ dimensions by projecting it onto the hyperplane defined by the first $d$ principal components.\n",
    "\n",
    "- Selecting this hyperplane ensures that the projection will preserve as much variance as possible.\n",
    "\n",
    "- To project the training set onto the hyperplane and obtain a reduced dataset $\\mathbf{X}_{d\\text{-proj}}$ of dimensionality $d$, compute the matrix multiplication of the training set matrix $\\mathbf{X}$ by the matrix $\\mathbf{W}_d$ , defined as the matrix containing the first $d$ columns of $\\mathbf{V}$, as shown in Equation 8-2."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "**Equation 8-2: Projecting the training set down to _d_ dimensions**\n",
    "\n",
    "$\n",
    "\\mathbf{X}_{d\\text{-proj}} = \\mathbf{X} \\mathbf{W}_d\n",
    "$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h1><center>Using Scikit-Learn</center></h1>\n",
    "\n",
    "- Scikit-Learn’s PCA class uses SVD decomposition to implement PCA.\n",
    "\n",
    "- The following code applies PCA to reduce the dimensionality of the dataset down to two dimensions (note that it automatically takes care of centering the data):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "np.random.seed(4)\n",
    "m = 60\n",
    "w1, w2 = 0.1, 0.3\n",
    "noise = 0.1\n",
    "\n",
    "angles = np.random.rand(m) * 3 * np.pi / 2 - 0.5\n",
    "X = np.empty((m, 3))\n",
    "X[:, 0] = np.cos(angles) + np.sin(angles)/2 + noise * np.random.randn(m) / 2\n",
    "X[:, 1] = np.sin(angles) * 0.7 + noise * np.random.randn(m) / 2\n",
    "X[:, 2] = X[:, 0] * w1 + X[:, 1] * w2 + noise * np.random.randn(m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "pca = PCA(n_components = 2)\n",
    "X2D = pca.fit_transform(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- After fitting the PCA transformer to the dataset, its `components_` attribute holds the transpose of $\\mathbf{W}_d$ e.g., the unit vector that defines the first principal component is equal to `pca.components_.T[:, 0]`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h1><center>Explained Variance Ratio</center></h1>\n",
    "\n",
    "- Another useful piece of information is the explained variance ratio of each principal component, available via the `explained_variance_ratio_` variable.\n",
    "\n",
    "- The ratio indicates the proportion of the dataset’s variance that lies along each principal component."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.84248607, 0.14631839])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pca.explained_variance_ratio_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- This output tells you that 84.2% of the dataset’s variance lies along the first PC, and 14.6% lies along the second PC. This leaves less than 1.2% for the third PC, so it is reasonable to assume that the third PC probably carries little information."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h1><center>Choosing the Right Number of Dimensions</center></h1>\n",
    "\n",
    "- Instead of arbitrarily choosing the number of dimensions to reduce down to, it is simpler to choose the number of dimensions that add up to a sufficiently large portion of the variance (e.g., 95%).\n",
    "\n",
    "- Unless, of course, you are reducing dimensionality for data visualization — in that case you will want to reduce the dimensionality down to 2 or 3.\n",
    "\n",
    "- The following code performs PCA without reducing dimensionality, then computes the minimum number of dimensions required to preserve 95% of the training set’s variance:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "pca = PCA()\n",
    "pca.fit(X_train)\n",
    "cumsum = np.cumsum(pca.explained_variance_ratio_)\n",
    "d = np.argmax(cumsum >= 0.95) + 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- You could then set `n_components=d` and run PCA again. But there is a much better option: instead of specifying the number of principal components you want to preserve, you can set `n_components` to be a float between 0.0 and 1.0, indicating the ratio of variance you wish to preserve:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "pca = PCA(n_components=0.95)\n",
    "X_reduced = pca.fit_transform(X_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h1><center>Explained Variance Ratio</center></h1>\n",
    "\n",
    "- Yet another option is to plot the explained variance as a function of the number of dimensions (simply plot cumsum ; see Figure 8-8).\n",
    "\n",
    "- There will usually be an elbow in the curve, where the explained variance stops growing fast. In this case, you can see that reducing the dimensionality down to about 100 dimensions wouldn’t lose too much explained variance.\n",
    "\n",
    "<center><img src=\"img/fig-8-8.png\" align=\"center\"/></center>\n",
    "\n",
    "<font size='1'>Image from Ref[1]</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h1><center>PCA for Compression - Reconstruction Error</center></h1>\n",
    "\n",
    "- After dimensionality reduction, the training set takes up much less space. As an example, try applying PCA to the MNIST dataset while preserving 95% of its variance. You should find that each instance will have just over 150 features, instead of the original 784 features.\n",
    "\n",
    "- So, while most of the variance is preserved, the dataset is now less than 20% of its original size! This is a reasonable compression ratio, and you can see how this size reduction can speed up a classification algorithm (such as an SVM classifier) tremendously.\n",
    "\n",
    "- It is also possible to decompress the reduced dataset back to 784 dimensions by applying the inverse transformation of the PCA projection. This won’t give you back the original data, since the projection lost a bit of information (within the 5% variance that was dropped), but it will likely be close to the original data.\n",
    "\n",
    "- The mean squared distance between the original data and the reconstructed data (compressed and then decompressed) is called the **reconstruction error**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h1><center>Compression of MNIST</center></h1>\n",
    "\n",
    "- Figure 8-9 shows a few digits from the original training set (on the left), and the corresponding digits after compression and decompression.\n",
    "- You can see that there is a slight image quality loss, but the digits are still mostly intact.\n",
    "\n",
    "<center><img src=\"img/fig-8-9.png\" align=\"center\"/></center>\n",
    "\n",
    "<font size='1'>Slide from Ref[1]</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h1><center>Randomized PCA</center></h1>\n",
    "\n",
    "- If you set the `svd_solver` hyperparameter to `\"randomized\"` , Scikit-Learn uses a stochastic algorithm called **Randomized PCA** that quickly finds an approximation of the first $d$ principal components.\n",
    "\n",
    "- Its computational complexity is $O(m × d^2) + O(d^3)$, instead of $O(m × n^2) + O(n^3)$ for the full SVD approach, so it is dramatically faster than full SVD when $d$ is much smaller than $n$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "rnd_pca = PCA(n_components=154, svd_solver=\"randomized\")\n",
    "X_reduced = rnd_pca.fit_transform(X_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- By default, `svd_solver` is actually set to `\"auto\"` : Scikit-Learn automatically uses the randomized PCA algorithm if $m$ or $n$ is greater than 500 and $d$ is less than 80% of $m$ or $n$, or else it uses the full SVD approach.\n",
    "\n",
    "- If you want to force Scikit-Learn to use full SVD, you can set the `svd_solver` hyperparameter to `\"full\"`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h1><center>Incremental PCA</center></h1>\n",
    "\n",
    "- One problem with the preceding implementations of PCA is that they require the whole training set to fit in memory in order for the algorithm to run.\n",
    "\n",
    "\n",
    "- Fortunately, **Incremental PCA (IPCA)** algorithms have been developed. They allow you to split the training set into mini-batches and feed an **IPCA** algorithm one mini-batch at a time.\n",
    "\n",
    "\n",
    "- This is useful for large training sets and for applying PCA online (i.e., on the fly, as new instances arrive)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h1><center>Kernel PCA</center></h1>\n",
    "\n",
    "- With PCA, the implicit assumption is that data lies on a linear subspace. There are situations where this assumption might be violated.\n",
    "\n",
    "- It turns out that the same kernel trick used for SVM can be applied to PCA, making it possible to perform complex nonlinear projections for dimensionality reduction. This is called **Kernel PCA (kPCA)**.\n",
    "\n",
    "- **kPCA** is often good at preserving clusters of instances after projection, or sometimes even unrolling datasets that lie close to a twisted manifold.\n",
    "\n",
    "- **kPCA** makes it possible to perform complex nonlinear projections for dimensionality reduction."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h1><center>kPCA</center></h1>\n",
    "\n",
    "- The following code uses Scikit-Learn’s KernelPCA class to perform kPCA with an RBF kernel:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.decomposition import KernelPCA\n",
    "rbf_pca = KernelPCA(n_components = 2, kernel=\"rbf\", gamma=0.04)\n",
    "X_reduced = rbf_pca.fit_transform(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Figure 8-10 shows the Swiss roll, reduced to two dimensions using a linear kernel (equivalent to simply using the PCA class), an RBF kernel, and a sigmoid kernel.\n",
    "\n",
    "<center><img src=\"img/fig-8-10.png\" align=\"center\"/></center>\n",
    "\n",
    "<font size='1'>Image from Ref[1]</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h1><center>Selecting a Kernel and Tuning Hyperparameters</center></h1>\n",
    "\n",
    "- As kPCA is an unsupervised learning algorithm, there is no obvious performance measure to help you select the best kernel and hyperparameter values.\n",
    "\n",
    "- That said, dimensionality reduction is often a preparation step for a supervised learning task (e.g., classification), so you can use grid search to select the kernel and hyperparameters that lead to the best performance on that task.\n",
    "\n",
    "- The following code creates a two-step pipeline, first reducing dimensionality to two dimensions using kPCA, then applying Logistic Regression for classification. Then it uses `GridSearchCV` to find the best kernel and gamma value for kPCA in order to get the best classification accuracy at the end of the pipeline:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "clf = Pipeline([\n",
    "(\"kpca\", KernelPCA(n_components=2)),\n",
    "(\"log_reg\", LogisticRegression())\n",
    "])\n",
    "param_grid = [{\n",
    "\"kpca__gamma\": np.linspace(0.03, 0.05, 10),\n",
    "\"kpca__kernel\": [\"rbf\", \"sigmoid\"]\n",
    "}]\n",
    "grid_search = GridSearchCV(clf, param_grid, cv=3)\n",
    "grid_search.fit(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- The best kernel and hyperparameters are then available through the `best_params_` variable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h1><center>LLE</center></h1>\n",
    "\n",
    "- **Locally Linear Embedding (LLE)** is another powerful nonlinear dimensionality reduction (**NLDR**) technique.\n",
    "\n",
    "- It is a Manifold Learning technique that does not rely on projections, like the previous algorithms do.\n",
    "\n",
    "- In a nutshell, **LLE** works by first measuring how each training instance linearly relates to its closest neighbors (c.n.), and then looking for a low-dimensional representation of the training set where these local relationships are best preserved.\n",
    "\n",
    "- This approach makes it particularly good at unrolling twisted manifolds, especially when there is not too much noise.\n",
    "\n",
    "<center><img src=\"img/fig-8-12.png\" align=\"center\"/></center>\n",
    "\n",
    "<font size='1'>Image from Ref[1]</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h1><center>Other Dimensionality Reduction Techniques</center></h1>\n",
    "\n",
    "- There are many other dimensionality reduction techniques, several of which are available in Scikit-Learn. Here are some of the most popular ones: \n",
    "    - Random Projections\n",
    "    - Multidimensional Scaling (MDS)\n",
    "    - Isomap\n",
    "    - t-Distributed Stochastic Neighbor Embedding (t-SNE)\n",
    "    - Linear Discriminant Analysis (LDA)\n",
    "\n",
    "<center><img src=\"img/fig-8-13.png\" align=\"center\"/></center>\n",
    "\n",
    "<font size='1'>Image from Ref[1]</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h1><center>Next Topic: Unsupervised Learning & Clustering</center></h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "<h1><center>References</center></h1>\n",
    "\n",
    "[1] Hands-On ML Textbook Edition-2 2019\n",
    "\n",
    "[2] PCA - [Slides from EPFL](http://lasa.epfl.ch/teaching/lectures/ML_Msc/Slides/PCA.pdf)\n",
    "\n",
    "[3] PCA - [Matt Gormley's Slides CMU](https://www.cs.cmu.edu/~mgormley/courses/10701-f16/slides/lecture14-pca.pdf) (some originally from Andrew Ng's Slides, Poczos' Slides and Nina Balcan's Slides)\n",
    "\n",
    "[4] PCA - [Benrstein's Slides NYU](https://davidrosenberg.github.io/mlcourse/Archive/2017/Lectures/13-PCA-Slides.pdf)\n",
    "\n",
    "[5] PCA - [Rosasco's Slides MIT](http://lcsl.mit.edu/courses/mlcc/mlcc2015/slides/MLCC_05_PCA.pdf)\n",
    "\n",
    "[6] Introduction to fMRI - [University of Oxford](https://www.ndcn.ox.ac.uk/divisions/fmrib/what-is-fmri/introduction-to-fmri)\n",
    "\n",
    "[7] [PCA Explained Simply](https://blog.bioturing.com/2018/06/14/principal-component-analysis-explained-simply/)"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
